{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ViTRegressor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mViTRegressor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViTRegressor\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mTreeDataset\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mv2\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ViTRegressor'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ViTRegressor import ViTRegressor\n",
    "import TreeDataset\n",
    "import torchvision.transforms.v2 as v2\n",
    "from utils import channel_means_stds\n",
    "from math import sqrt\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #usa una GPU se disponibile\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\") # \n",
    "\n",
    "    #Iperparametri\n",
    "    num_epochs = 5\n",
    "    batch_size = 30\n",
    "    learning_rate = 1e-4\n",
    "    loss_function = nn.MSELoss()\n",
    "    params = {\n",
    "    'num_channels': 11,\n",
    "    'num_hidden_layers': 1, \n",
    "    'size_hidden_layers': [256],\n",
    "    'dropout_rates': [0.5, 0.5]  \n",
    "    }\n",
    "    # Modello\n",
    "    model = ViTRegressor(params['num_hidden_layers'], params['size_hidden_layers'], params['dropout_rates']).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Crea dataset\n",
    "    train_set = TreeDataset.TreeDataset(mode='train')\n",
    "    val_set = TreeDataset.TreeDataset(mode='val')\n",
    "    test_set = TreeDataset.TreeDataset(mode='test')\n",
    "\n",
    "    #calcolo medie e stds per canale del train_set\n",
    "    train_means, train_stds = channel_means_stds(train_set)\n",
    "\n",
    "    '''\n",
    "    (0.485, 0.456, 0.406) imagenet means (red, green, blue)\n",
    "    (0.229, 0.224, 0.225) imagenet std (rgb)\n",
    "    sotto ho seguito lo schema dell'ordine delle bande nei tensori\n",
    "    means = (0.406, 0.406, 0.456, 0.456, 0.456, 0.485, 0.456, 0.456, 0.456, 0.485, 0.456)\n",
    "    stds = (0.225, 0.225, 0.224, 0.224, 0.224, 0.229, 0.224, 0.224, 0.224, 0.229, 0.224)\n",
    "    '''\n",
    "\n",
    "    #Data augmentation + train transforms\n",
    "    train_transforms = v2.Compose([\n",
    "                        v2.Resize((224, 224), antialias=True),\n",
    "                        v2.RandomCrop(180),\n",
    "                        v2.Resize((224, 224), antialias=True),\n",
    "                        v2.RandomHorizontalFlip(),\n",
    "                        v2.RandomVerticalFlip(),\n",
    "                        #transforms.RandomApply([transforms.GaussianBlur(3, sigma=(0.1, 2.0))], p=0.5),\n",
    "                        v2.ToTensor(),\n",
    "                        v2.Normalize(train_means, train_stds)])\n",
    "\n",
    "    #val/test normalization\n",
    "    test_transforms = v2.Compose([\n",
    "                        v2.Resize((224, 224), antialias=True),\n",
    "                        v2.ToTensor(),\n",
    "                        v2.Normalize(train_means, train_stds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17715.6875, 16900.486328125, 19324.916015625, 18412.6015625, 33659.69140625, 16722.482421875, 20676.611328125, 30762.6875, 26453.736328125, 16695.986328125, 43.55046844482422]\n",
      "[10419.953125, 10096.2568359375, 10045.6787109375, 9415.35546875, 8845.2705078125, 11132.8017578125, 10644.0224609375, 9388.4013671875, 9778.1474609375, 11603.029296875, 7.6262030601501465]\n"
     ]
    }
   ],
   "source": [
    "train_set = TreeDataset.TreeDataset(mode='train')\n",
    "train_means, train_stds = channel_means_stds(train_set)\n",
    "print(train_means)\n",
    "print(train_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "data = pd.read_csv(r'C:\\Users\\giova\\OneDrive\\Desktop\\VS Code folders\\SWP-regr\\data\\train_set.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giova\\OneDrive\\Desktop\\VS Code folders\\SWP-regr\\data\\DATASET\\train\\11_3 Zeuli 220623 Albero 32.tif 8.8\n"
     ]
    }
   ],
   "source": [
    "img_path = data['img_path'][0]\n",
    "img_label = data[' SWP'][0]\n",
    "print(img_path, img_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = rio.open(img_path)\n",
    "img_data = img.read()\n",
    "img_tensor = torch.from_numpy(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 224, 224])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giova\\OneDrive\\Desktop\\VS Code folders\\env_prova\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_transforms = v2.Compose([\n",
    "                v2.Resize((224, 224), antialias=True),\n",
    "                transforms.RandomApply([v2.RandomCrop(180)], p=1),\n",
    "                v2.Resize((224, 224), antialias=True),\n",
    "                v2.RandomHorizontalFlip(p=1),\n",
    "                v2.RandomVerticalFlip(p=1),\n",
    "                #transforms.RandomApply([transforms.GaussianBlur(3, sigma=(0.1, 2.0))], p=0.5),\n",
    "                v2.ToTensor(),\n",
    "                v2.Normalize(train_means, train_stds)])\n",
    "\n",
    "new_img_tensor = train_transforms(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_img_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_channels': 11,\n",
    "    'num_hidden_layers': 1, \n",
    "    'size_hidden_layers': [256],\n",
    "    'dropout_rates': [0.5]  \n",
    "    }\n",
    "# Modello\n",
    "model = ViTRegressor(params['num_hidden_layers'], params['size_hidden_layers'], params['dropout_rates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "  print(p.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_dropout_rates_0</th>\n",
       "      <th>params_dropout_rates_1</th>\n",
       "      <th>params_dropout_rates_2</th>\n",
       "      <th>params_dropout_rates_3</th>\n",
       "      <th>params_dropout_rates_4</th>\n",
       "      <th>...</th>\n",
       "      <th>params_neurons_per_layer_4</th>\n",
       "      <th>params_neurons_per_layer_5</th>\n",
       "      <th>params_neurons_per_layer_6</th>\n",
       "      <th>params_neurons_per_layer_7</th>\n",
       "      <th>params_neurons_per_layer_8</th>\n",
       "      <th>params_neurons_per_layer_9</th>\n",
       "      <th>params_num_hidden_layers</th>\n",
       "      <th>user_attrs_batch_size</th>\n",
       "      <th>user_attrs_n_epochs</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>417</td>\n",
       "      <td>7.463090</td>\n",
       "      <td>2024-05-09 02:22:23.219708</td>\n",
       "      <td>2024-05-09 02:23:29.174814</td>\n",
       "      <td>0 days 00:01:05.955106</td>\n",
       "      <td>0.373490</td>\n",
       "      <td>0.557378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>1820</td>\n",
       "      <td>10.659382</td>\n",
       "      <td>2024-05-10 03:57:58.013850</td>\n",
       "      <td>2024-05-10 03:58:52.795791</td>\n",
       "      <td>0 days 00:00:54.781941</td>\n",
       "      <td>0.594085</td>\n",
       "      <td>0.504948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>1146</td>\n",
       "      <td>11.432973</td>\n",
       "      <td>2024-05-09 17:27:03.783482</td>\n",
       "      <td>2024-05-09 17:27:58.326695</td>\n",
       "      <td>0 days 00:00:54.543213</td>\n",
       "      <td>0.548766</td>\n",
       "      <td>0.564343</td>\n",
       "      <td>0.728063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>482</td>\n",
       "      <td>12.017195</td>\n",
       "      <td>2024-05-09 03:28:56.445119</td>\n",
       "      <td>2024-05-09 03:29:56.493009</td>\n",
       "      <td>0 days 00:01:00.047890</td>\n",
       "      <td>0.426541</td>\n",
       "      <td>0.557086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>1392</td>\n",
       "      <td>12.650881</td>\n",
       "      <td>2024-05-09 21:27:19.990757</td>\n",
       "      <td>2024-05-09 21:27:50.805636</td>\n",
       "      <td>0 days 00:00:30.814879</td>\n",
       "      <td>0.378439</td>\n",
       "      <td>0.495113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>390</td>\n",
       "      <td>13.085997</td>\n",
       "      <td>2024-05-09 02:00:32.712530</td>\n",
       "      <td>2024-05-09 02:01:22.815546</td>\n",
       "      <td>0 days 00:00:50.103016</td>\n",
       "      <td>0.464244</td>\n",
       "      <td>0.557958</td>\n",
       "      <td>0.448935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>1325</td>\n",
       "      <td>14.024838</td>\n",
       "      <td>2024-05-09 20:21:22.790093</td>\n",
       "      <td>2024-05-09 20:22:19.196275</td>\n",
       "      <td>0 days 00:00:56.406182</td>\n",
       "      <td>0.447407</td>\n",
       "      <td>0.635112</td>\n",
       "      <td>0.234421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>399</td>\n",
       "      <td>14.482193</td>\n",
       "      <td>2024-05-09 02:05:44.869575</td>\n",
       "      <td>2024-05-09 02:06:15.764153</td>\n",
       "      <td>0 days 00:00:30.894578</td>\n",
       "      <td>0.383724</td>\n",
       "      <td>0.492117</td>\n",
       "      <td>0.288089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>755</td>\n",
       "      <td>16.198002</td>\n",
       "      <td>2024-05-09 10:59:45.363278</td>\n",
       "      <td>2024-05-09 11:00:38.720813</td>\n",
       "      <td>0 days 00:00:53.357535</td>\n",
       "      <td>0.345532</td>\n",
       "      <td>0.591869</td>\n",
       "      <td>0.586896</td>\n",
       "      <td>0.738326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>1237</td>\n",
       "      <td>16.264519</td>\n",
       "      <td>2024-05-09 18:54:00.768036</td>\n",
       "      <td>2024-05-09 18:54:44.254685</td>\n",
       "      <td>0 days 00:00:43.486649</td>\n",
       "      <td>0.411130</td>\n",
       "      <td>0.588447</td>\n",
       "      <td>0.401186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>1817</td>\n",
       "      <td>16.340986</td>\n",
       "      <td>2024-05-10 03:56:31.187816</td>\n",
       "      <td>2024-05-10 03:57:14.639665</td>\n",
       "      <td>0 days 00:00:43.451849</td>\n",
       "      <td>0.550402</td>\n",
       "      <td>0.524198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>1630</td>\n",
       "      <td>16.409322</td>\n",
       "      <td>2024-05-10 01:15:10.287304</td>\n",
       "      <td>2024-05-10 01:16:03.128765</td>\n",
       "      <td>0 days 00:00:52.841461</td>\n",
       "      <td>0.526300</td>\n",
       "      <td>0.624783</td>\n",
       "      <td>0.213808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>1946</td>\n",
       "      <td>16.640729</td>\n",
       "      <td>2024-05-10 05:49:02.354712</td>\n",
       "      <td>2024-05-10 05:50:01.079553</td>\n",
       "      <td>0 days 00:00:58.724841</td>\n",
       "      <td>0.558076</td>\n",
       "      <td>0.672308</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>1497</td>\n",
       "      <td>16.787665</td>\n",
       "      <td>2024-05-09 23:02:39.188517</td>\n",
       "      <td>2024-05-09 23:03:39.302557</td>\n",
       "      <td>0 days 00:01:00.114040</td>\n",
       "      <td>0.356268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>322</td>\n",
       "      <td>17.217227</td>\n",
       "      <td>2024-05-09 01:12:58.120616</td>\n",
       "      <td>2024-05-09 01:13:40.069685</td>\n",
       "      <td>0 days 00:00:41.949069</td>\n",
       "      <td>0.412385</td>\n",
       "      <td>0.599341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>17.322991</td>\n",
       "      <td>2024-05-08 18:51:24.436155</td>\n",
       "      <td>2024-05-08 18:53:45.089834</td>\n",
       "      <td>0 days 00:02:20.653679</td>\n",
       "      <td>0.385577</td>\n",
       "      <td>0.316816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>852</td>\n",
       "      <td>17.355948</td>\n",
       "      <td>2024-05-09 12:55:48.819141</td>\n",
       "      <td>2024-05-09 12:56:38.217654</td>\n",
       "      <td>0 days 00:00:49.398513</td>\n",
       "      <td>0.396313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>1061</td>\n",
       "      <td>17.414607</td>\n",
       "      <td>2024-05-09 16:02:47.076412</td>\n",
       "      <td>2024-05-09 16:03:20.802023</td>\n",
       "      <td>0 days 00:00:33.725611</td>\n",
       "      <td>0.395954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>217</td>\n",
       "      <td>17.473267</td>\n",
       "      <td>2024-05-08 23:29:20.148087</td>\n",
       "      <td>2024-05-08 23:30:10.140756</td>\n",
       "      <td>0 days 00:00:49.992669</td>\n",
       "      <td>0.390486</td>\n",
       "      <td>0.608921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>1200</td>\n",
       "      <td>17.767274</td>\n",
       "      <td>2024-05-09 18:19:59.933332</td>\n",
       "      <td>2024-05-09 18:20:45.694893</td>\n",
       "      <td>0 days 00:00:45.761561</td>\n",
       "      <td>0.472610</td>\n",
       "      <td>0.556833</td>\n",
       "      <td>0.559740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      number      value             datetime_start          datetime_complete  \\\n",
       "417      417   7.463090 2024-05-09 02:22:23.219708 2024-05-09 02:23:29.174814   \n",
       "1820    1820  10.659382 2024-05-10 03:57:58.013850 2024-05-10 03:58:52.795791   \n",
       "1146    1146  11.432973 2024-05-09 17:27:03.783482 2024-05-09 17:27:58.326695   \n",
       "482      482  12.017195 2024-05-09 03:28:56.445119 2024-05-09 03:29:56.493009   \n",
       "1392    1392  12.650881 2024-05-09 21:27:19.990757 2024-05-09 21:27:50.805636   \n",
       "390      390  13.085997 2024-05-09 02:00:32.712530 2024-05-09 02:01:22.815546   \n",
       "1325    1325  14.024838 2024-05-09 20:21:22.790093 2024-05-09 20:22:19.196275   \n",
       "399      399  14.482193 2024-05-09 02:05:44.869575 2024-05-09 02:06:15.764153   \n",
       "755      755  16.198002 2024-05-09 10:59:45.363278 2024-05-09 11:00:38.720813   \n",
       "1237    1237  16.264519 2024-05-09 18:54:00.768036 2024-05-09 18:54:44.254685   \n",
       "1817    1817  16.340986 2024-05-10 03:56:31.187816 2024-05-10 03:57:14.639665   \n",
       "1630    1630  16.409322 2024-05-10 01:15:10.287304 2024-05-10 01:16:03.128765   \n",
       "1946    1946  16.640729 2024-05-10 05:49:02.354712 2024-05-10 05:50:01.079553   \n",
       "1497    1497  16.787665 2024-05-09 23:02:39.188517 2024-05-09 23:03:39.302557   \n",
       "322      322  17.217227 2024-05-09 01:12:58.120616 2024-05-09 01:13:40.069685   \n",
       "78        78  17.322991 2024-05-08 18:51:24.436155 2024-05-08 18:53:45.089834   \n",
       "852      852  17.355948 2024-05-09 12:55:48.819141 2024-05-09 12:56:38.217654   \n",
       "1061    1061  17.414607 2024-05-09 16:02:47.076412 2024-05-09 16:03:20.802023   \n",
       "217      217  17.473267 2024-05-08 23:29:20.148087 2024-05-08 23:30:10.140756   \n",
       "1200    1200  17.767274 2024-05-09 18:19:59.933332 2024-05-09 18:20:45.694893   \n",
       "\n",
       "                   duration  params_dropout_rates_0  params_dropout_rates_1  \\\n",
       "417  0 days 00:01:05.955106                0.373490                0.557378   \n",
       "1820 0 days 00:00:54.781941                0.594085                0.504948   \n",
       "1146 0 days 00:00:54.543213                0.548766                0.564343   \n",
       "482  0 days 00:01:00.047890                0.426541                0.557086   \n",
       "1392 0 days 00:00:30.814879                0.378439                0.495113   \n",
       "390  0 days 00:00:50.103016                0.464244                0.557958   \n",
       "1325 0 days 00:00:56.406182                0.447407                0.635112   \n",
       "399  0 days 00:00:30.894578                0.383724                0.492117   \n",
       "755  0 days 00:00:53.357535                0.345532                0.591869   \n",
       "1237 0 days 00:00:43.486649                0.411130                0.588447   \n",
       "1817 0 days 00:00:43.451849                0.550402                0.524198   \n",
       "1630 0 days 00:00:52.841461                0.526300                0.624783   \n",
       "1946 0 days 00:00:58.724841                0.558076                0.672308   \n",
       "1497 0 days 00:01:00.114040                0.356268                     NaN   \n",
       "322  0 days 00:00:41.949069                0.412385                0.599341   \n",
       "78   0 days 00:02:20.653679                0.385577                0.316816   \n",
       "852  0 days 00:00:49.398513                0.396313                     NaN   \n",
       "1061 0 days 00:00:33.725611                0.395954                     NaN   \n",
       "217  0 days 00:00:49.992669                0.390486                0.608921   \n",
       "1200 0 days 00:00:45.761561                0.472610                0.556833   \n",
       "\n",
       "      params_dropout_rates_2  params_dropout_rates_3  params_dropout_rates_4  \\\n",
       "417                      NaN                     NaN                     NaN   \n",
       "1820                     NaN                     NaN                     NaN   \n",
       "1146                0.728063                     NaN                     NaN   \n",
       "482                      NaN                     NaN                     NaN   \n",
       "1392                     NaN                     NaN                     NaN   \n",
       "390                 0.448935                     NaN                     NaN   \n",
       "1325                0.234421                     NaN                     NaN   \n",
       "399                 0.288089                     NaN                     NaN   \n",
       "755                 0.586896                0.738326                     NaN   \n",
       "1237                0.401186                     NaN                     NaN   \n",
       "1817                     NaN                     NaN                     NaN   \n",
       "1630                0.213808                     NaN                     NaN   \n",
       "1946                     NaN                     NaN                     NaN   \n",
       "1497                     NaN                     NaN                     NaN   \n",
       "322                      NaN                     NaN                     NaN   \n",
       "78                       NaN                     NaN                     NaN   \n",
       "852                      NaN                     NaN                     NaN   \n",
       "1061                     NaN                     NaN                     NaN   \n",
       "217                      NaN                     NaN                     NaN   \n",
       "1200                0.559740                     NaN                     NaN   \n",
       "\n",
       "      ...  params_neurons_per_layer_4  params_neurons_per_layer_5  \\\n",
       "417   ...                         NaN                         NaN   \n",
       "1820  ...                         NaN                         NaN   \n",
       "1146  ...                         NaN                         NaN   \n",
       "482   ...                         NaN                         NaN   \n",
       "1392  ...                         NaN                         NaN   \n",
       "390   ...                         NaN                         NaN   \n",
       "1325  ...                         NaN                         NaN   \n",
       "399   ...                         NaN                         NaN   \n",
       "755   ...                         NaN                         NaN   \n",
       "1237  ...                         NaN                         NaN   \n",
       "1817  ...                         NaN                         NaN   \n",
       "1630  ...                         NaN                         NaN   \n",
       "1946  ...                         NaN                         NaN   \n",
       "1497  ...                         NaN                         NaN   \n",
       "322   ...                         NaN                         NaN   \n",
       "78    ...                         NaN                         NaN   \n",
       "852   ...                         NaN                         NaN   \n",
       "1061  ...                         NaN                         NaN   \n",
       "217   ...                         NaN                         NaN   \n",
       "1200  ...                         NaN                         NaN   \n",
       "\n",
       "      params_neurons_per_layer_6  params_neurons_per_layer_7  \\\n",
       "417                          NaN                         NaN   \n",
       "1820                         NaN                         NaN   \n",
       "1146                         NaN                         NaN   \n",
       "482                          NaN                         NaN   \n",
       "1392                         NaN                         NaN   \n",
       "390                          NaN                         NaN   \n",
       "1325                         NaN                         NaN   \n",
       "399                          NaN                         NaN   \n",
       "755                          NaN                         NaN   \n",
       "1237                         NaN                         NaN   \n",
       "1817                         NaN                         NaN   \n",
       "1630                         NaN                         NaN   \n",
       "1946                         NaN                         NaN   \n",
       "1497                         NaN                         NaN   \n",
       "322                          NaN                         NaN   \n",
       "78                           NaN                         NaN   \n",
       "852                          NaN                         NaN   \n",
       "1061                         NaN                         NaN   \n",
       "217                          NaN                         NaN   \n",
       "1200                         NaN                         NaN   \n",
       "\n",
       "      params_neurons_per_layer_8  params_neurons_per_layer_9  \\\n",
       "417                          NaN                         NaN   \n",
       "1820                         NaN                         NaN   \n",
       "1146                         NaN                         NaN   \n",
       "482                          NaN                         NaN   \n",
       "1392                         NaN                         NaN   \n",
       "390                          NaN                         NaN   \n",
       "1325                         NaN                         NaN   \n",
       "399                          NaN                         NaN   \n",
       "755                          NaN                         NaN   \n",
       "1237                         NaN                         NaN   \n",
       "1817                         NaN                         NaN   \n",
       "1630                         NaN                         NaN   \n",
       "1946                         NaN                         NaN   \n",
       "1497                         NaN                         NaN   \n",
       "322                          NaN                         NaN   \n",
       "78                           NaN                         NaN   \n",
       "852                          NaN                         NaN   \n",
       "1061                         NaN                         NaN   \n",
       "217                          NaN                         NaN   \n",
       "1200                         NaN                         NaN   \n",
       "\n",
       "      params_num_hidden_layers  user_attrs_batch_size  user_attrs_n_epochs  \\\n",
       "417                          2                      5                  500   \n",
       "1820                         2                      5                  500   \n",
       "1146                         3                      5                  500   \n",
       "482                          2                      5                  500   \n",
       "1392                         2                      5                  500   \n",
       "390                          3                      5                  500   \n",
       "1325                         3                      5                  500   \n",
       "399                          3                      5                  500   \n",
       "755                          4                      5                  500   \n",
       "1237                         3                      5                  500   \n",
       "1817                         2                      5                  500   \n",
       "1630                         3                      5                  500   \n",
       "1946                         2                      5                  500   \n",
       "1497                         1                      5                  500   \n",
       "322                          2                      5                  500   \n",
       "78                           2                      5                  500   \n",
       "852                          1                      5                  500   \n",
       "1061                         1                      5                  500   \n",
       "217                          2                      5                  500   \n",
       "1200                         3                      5                  500   \n",
       "\n",
       "         state  \n",
       "417   COMPLETE  \n",
       "1820  COMPLETE  \n",
       "1146  COMPLETE  \n",
       "482   COMPLETE  \n",
       "1392  COMPLETE  \n",
       "390   COMPLETE  \n",
       "1325  COMPLETE  \n",
       "399   COMPLETE  \n",
       "755   COMPLETE  \n",
       "1237  COMPLETE  \n",
       "1817  COMPLETE  \n",
       "1630  COMPLETE  \n",
       "1946  COMPLETE  \n",
       "1497  COMPLETE  \n",
       "322   COMPLETE  \n",
       "78    COMPLETE  \n",
       "852   COMPLETE  \n",
       "1061  COMPLETE  \n",
       "217   COMPLETE  \n",
       "1200  COMPLETE  \n",
       "\n",
       "[20 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study = joblib.load(r'C:\\Users\\giova\\OneDrive\\Desktop\\projects\\SWP-regr\\optuna_results.pkl')\n",
    "df = study.trials_dataframe()\n",
    "df.sort_values('value').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: 417\n",
      "    MSE: 7.46308970451355\n",
      "    RMSE: 2.7318656087943913\n",
      "    params: {'num_hidden_layers': 2, 'neurons_per_layer_0': 519, 'neurons_per_layer_1': 948, 'learning_rate': 0.0005, 'dropout_rates_0': 0.37349024149018334, 'dropout_rates_1': 0.5573776200668319}\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "print(f\"Best trial: {study.best_trial.number}\")\n",
    "print(f\"    MSE: {study.best_trial.value}\")\n",
    "print(f\"    RMSE: {sqrt(study.best_trial.value)}\")\n",
    "print(f\"    params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.sort_values('value').head(1900)['value'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_prova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
